{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "TARGET_SIZE: Tuple[int, int] = (24, 24)\n",
    "\n",
    "\n",
    "#TARGET_SIZE: tuple[int, int] = (24, 24)\n",
    "COLOR_MODE: str = \"grayscale\"\n",
    "\n",
    "ACCURACY: float = 0.95\n",
    "IMAGE_SIZE: int = 24\n",
    "BATCH_SIZE: int = 32\n",
    "EPOCHS: int = 10\n",
    "\n",
    "\n",
    "def generator(dir, shuffle=True, batch_size=1, target_size=TARGET_SIZE, class_mode='categorical'):\n",
    "    return tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) \\\n",
    "        .flow_from_directory(dir, batch_size=batch_size, shuffle=shuffle,\n",
    "                             color_mode=COLOR_MODE, class_mode=class_mode, target_size=target_size)\n",
    "\n",
    "\n",
    "dirValid = \"C:/Users/user/Desktop/PDSEI 2023/Data Paper/ojos/test\"\n",
    "dirTrein = \"C:/Users/user/Desktop/PDSEI 2023/Data Paper/ojos/train\"\n",
    "\n",
    "\n",
    "model: tf.keras.models.Sequential = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(units=2, activation=\"sigmoid\")\n",
    "])\n",
    "    \n",
    "train_generator = generator(dirTrein, batch_size=BATCH_SIZE, target_size=TARGET_SIZE)\n",
    "valid_generator = generator(dirValid, batch_size=BATCH_SIZE, target_size=TARGET_SIZE)\n",
    "\n",
    "# Convertir los generadores en tensores numpy\n",
    "train_images, train_labels = next(train_generator)\n",
    "valid_images, valid_labels = next(valid_generator)\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_data=(valid_images, valid_labels),\n",
    "    epochs=EPOCHS,\n",
    "    shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1234 images belonging to 2 classes.\n",
      "Found 218 images belonging to 2 classes.\n",
      "Epoch 1/15\n",
      "39/39 [==============================] - 13s 339ms/step - loss: 0.6584 - acc: 0.6182 - val_loss: 0.4798 - val_acc: 0.8326\n",
      "Epoch 2/15\n",
      "39/39 [==============================] - 10s 251ms/step - loss: 0.4074 - acc: 0.8212 - val_loss: 0.2924 - val_acc: 0.8761\n",
      "Epoch 3/15\n",
      "39/39 [==============================] - 10s 245ms/step - loss: 0.2810 - acc: 0.8992 - val_loss: 0.2208 - val_acc: 0.9128\n",
      "Epoch 4/15\n",
      "39/39 [==============================] - 9s 241ms/step - loss: 0.2027 - acc: 0.9269 - val_loss: 0.1722 - val_acc: 0.9312\n",
      "Epoch 5/15\n",
      "39/39 [==============================] - 9s 235ms/step - loss: 0.1743 - acc: 0.9399 - val_loss: 0.1517 - val_acc: 0.9381\n",
      "Epoch 6/15\n",
      "39/39 [==============================] - 9s 240ms/step - loss: 0.1522 - acc: 0.9457 - val_loss: 0.1446 - val_acc: 0.9312\n",
      "Epoch 7/15\n",
      "39/39 [==============================] - 9s 242ms/step - loss: 0.1160 - acc: 0.9571 - val_loss: 0.1270 - val_acc: 0.9518\n",
      "Epoch 8/15\n",
      "39/39 [==============================] - 10s 256ms/step - loss: 0.0850 - acc: 0.9679 - val_loss: 0.1421 - val_acc: 0.9427\n",
      "Epoch 9/15\n",
      "39/39 [==============================] - 10s 250ms/step - loss: 0.0959 - acc: 0.9623 - val_loss: 0.0892 - val_acc: 0.9610\n",
      "Epoch 10/15\n",
      "39/39 [==============================] - 9s 242ms/step - loss: 0.0628 - acc: 0.9812 - val_loss: 0.0766 - val_acc: 0.9679\n",
      "Epoch 11/15\n",
      "39/39 [==============================] - 9s 237ms/step - loss: 0.0520 - acc: 0.9812 - val_loss: 0.0670 - val_acc: 0.9817\n",
      "Epoch 12/15\n",
      "39/39 [==============================] - 9s 240ms/step - loss: 0.0508 - acc: 0.9845 - val_loss: 0.0680 - val_acc: 0.9771\n",
      "Epoch 13/15\n",
      "39/39 [==============================] - 9s 240ms/step - loss: 0.0436 - acc: 0.9900 - val_loss: 0.0628 - val_acc: 0.9862\n",
      "Epoch 14/15\n",
      "39/39 [==============================] - 9s 240ms/step - loss: 0.0272 - acc: 0.9908 - val_loss: 0.0945 - val_acc: 0.9771\n",
      "Epoch 15/15\n",
      "39/39 [==============================] - 10s 259ms/step - loss: 0.0293 - acc: 0.9904 - val_loss: 0.0738 - val_acc: 0.9679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26f6671f630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Tuple\n",
    "\n",
    "TARGET_SIZE: Tuple[int, int] = (24, 24)\n",
    "COLOR_MODE: str = \"grayscale\"\n",
    "\n",
    "ACCURACY: float = 0.95\n",
    "IMAGE_SIZE: int = 24\n",
    "BATCH_SIZE: int = 32\n",
    "EPOCHS: int = 15\n",
    "\n",
    "def generator(dir, shuffle=True, batch_size=1, target_size=TARGET_SIZE, class_mode='categorical'):\n",
    "    return tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) \\\n",
    "        .flow_from_directory(dir, batch_size=batch_size, shuffle=shuffle,\n",
    "                             color_mode=COLOR_MODE, class_mode=class_mode, target_size=target_size)\n",
    "\n",
    "dirValid = \"C:/Users/user/Desktop/PDSEI 2023/Data Paper/ojos/test\"  # Cambiar por el directorio de tus im√°genes\n",
    "dirTrein = \"C:/Users/user/Desktop/PDSEI 2023/Data Paper/ojos/train\"\n",
    "\n",
    "train_generator = generator(dirTrein, batch_size=BATCH_SIZE, target_size=TARGET_SIZE)\n",
    "valid_generator = generator(dirValid, batch_size=BATCH_SIZE, target_size=TARGET_SIZE)\n",
    "\n",
    "model: tf.keras.models.Sequential = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(units=2, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=EPOCHS,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1234 images belonging to 2 classes.\n",
      "Found 218 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "39/39 [==============================] - 13s 341ms/step - loss: 0.6231 - acc: 0.5988 - val_loss: 0.4097 - val_acc: 0.8647\n",
      "Epoch 2/25\n",
      "39/39 [==============================] - 10s 258ms/step - loss: 0.3330 - acc: 0.8679 - val_loss: 0.2245 - val_acc: 0.9106\n",
      "Epoch 3/25\n",
      "39/39 [==============================] - 9s 242ms/step - loss: 0.2602 - acc: 0.8941 - val_loss: 0.1895 - val_acc: 0.9220\n",
      "Epoch 4/25\n",
      "39/39 [==============================] - 9s 243ms/step - loss: 0.2035 - acc: 0.9182 - val_loss: 0.1687 - val_acc: 0.9335\n",
      "Epoch 5/25\n",
      "39/39 [==============================] - 9s 240ms/step - loss: 0.1742 - acc: 0.9355 - val_loss: 0.1224 - val_acc: 0.9541\n",
      "Epoch 6/25\n",
      "39/39 [==============================] - 9s 238ms/step - loss: 0.1570 - acc: 0.9434 - val_loss: 0.1231 - val_acc: 0.9404\n",
      "Epoch 7/25\n",
      "39/39 [==============================] - 10s 244ms/step - loss: 0.0971 - acc: 0.9711 - val_loss: 0.0820 - val_acc: 0.9794\n",
      "Epoch 8/25\n",
      "39/39 [==============================] - 9s 243ms/step - loss: 0.0838 - acc: 0.9737 - val_loss: 0.0878 - val_acc: 0.9725\n",
      "Epoch 9/25\n",
      "39/39 [==============================] - 10s 258ms/step - loss: 0.0598 - acc: 0.9764 - val_loss: 0.0778 - val_acc: 0.9771\n",
      "Epoch 10/25\n",
      "39/39 [==============================] - 9s 241ms/step - loss: 0.0565 - acc: 0.9812 - val_loss: 0.0632 - val_acc: 0.9771\n",
      "Epoch 11/25\n",
      "39/39 [==============================] - 9s 241ms/step - loss: 0.0407 - acc: 0.9870 - val_loss: 0.0604 - val_acc: 0.9771\n",
      "Epoch 12/25\n",
      "39/39 [==============================] - 9s 231ms/step - loss: 0.0343 - acc: 0.9888 - val_loss: 0.0588 - val_acc: 0.9817\n",
      "Epoch 13/25\n",
      "39/39 [==============================] - 9s 234ms/step - loss: 0.0315 - acc: 0.9888 - val_loss: 0.0606 - val_acc: 0.9817\n",
      "Epoch 14/25\n",
      "39/39 [==============================] - 10s 244ms/step - loss: 0.0293 - acc: 0.9876 - val_loss: 0.0462 - val_acc: 0.9771\n",
      "Epoch 15/25\n",
      "39/39 [==============================] - 10s 256ms/step - loss: 0.0251 - acc: 0.9916 - val_loss: 0.0523 - val_acc: 0.9817\n",
      "Epoch 16/25\n",
      "39/39 [==============================] - 10s 255ms/step - loss: 0.0164 - acc: 0.9932 - val_loss: 0.0683 - val_acc: 0.9817\n",
      "Epoch 17/25\n",
      "39/39 [==============================] - 9s 238ms/step - loss: 0.0132 - acc: 0.9956 - val_loss: 0.0821 - val_acc: 0.9725\n",
      "Epoch 18/25\n",
      "39/39 [==============================] - 9s 239ms/step - loss: 0.0213 - acc: 0.9932 - val_loss: 0.0412 - val_acc: 0.9908\n",
      "Epoch 19/25\n",
      "39/39 [==============================] - 9s 233ms/step - loss: 0.0120 - acc: 0.9960 - val_loss: 0.0760 - val_acc: 0.9862\n",
      "Epoch 20/25\n",
      "39/39 [==============================] - 9s 241ms/step - loss: 0.0144 - acc: 0.9948 - val_loss: 0.0447 - val_acc: 0.9908\n",
      "Epoch 21/25\n",
      "39/39 [==============================] - 9s 236ms/step - loss: 0.0104 - acc: 0.9972 - val_loss: 0.0639 - val_acc: 0.9794\n",
      "Epoch 22/25\n",
      "39/39 [==============================] - 10s 253ms/step - loss: 0.0192 - acc: 0.9908 - val_loss: 0.0487 - val_acc: 0.9908\n",
      "Epoch 23/25\n",
      "39/39 [==============================] - 10s 254ms/step - loss: 0.0138 - acc: 0.9968 - val_loss: 0.0525 - val_acc: 0.9908\n",
      "Epoch 24/25\n",
      "39/39 [==============================] - 9s 239ms/step - loss: 0.0070 - acc: 0.9984 - val_loss: 0.0495 - val_acc: 0.9862\n",
      "Epoch 25/25\n",
      "39/39 [==============================] - 9s 238ms/step - loss: 0.0058 - acc: 0.9984 - val_loss: 0.0548 - val_acc: 0.9862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26f76131668>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Tuple\n",
    "\n",
    "TARGET_SIZE: Tuple[int, int] = (24, 24)\n",
    "COLOR_MODE: str = \"grayscale\"\n",
    "\n",
    "ACCURACY: float = 0.95\n",
    "IMAGE_SIZE: int = 24\n",
    "BATCH_SIZE: int = 32\n",
    "EPOCHS: int = 25\n",
    "\n",
    "def generator(dir, shuffle=True, batch_size=1, target_size=TARGET_SIZE, class_mode='categorical'):\n",
    "    return tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) \\\n",
    "        .flow_from_directory(dir, batch_size=batch_size, shuffle=shuffle,\n",
    "                             color_mode=COLOR_MODE, class_mode=class_mode, target_size=target_size)\n",
    "\n",
    "dirValid = \"C:/Users/user/Desktop/PDSEI 2023/Data Paper/ojos/test\"  # Cambiar por el directorio de tus im√°genes\n",
    "dirTrein = \"C:/Users/user/Desktop/PDSEI 2023/Data Paper/ojos/train\"\n",
    "\n",
    "train_generator = generator(dirTrein, batch_size=BATCH_SIZE, target_size=TARGET_SIZE)\n",
    "valid_generator = generator(dirValid, batch_size=BATCH_SIZE, target_size=TARGET_SIZE)\n",
    "\n",
    "model: tf.keras.models.Sequential = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(units=2, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=EPOCHS,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
